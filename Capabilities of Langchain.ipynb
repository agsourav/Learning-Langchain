{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb65178f",
   "metadata": {},
   "source": [
    "## Capabilities of Langchain\n",
    "\n",
    "Langchain is an AI framework that enables building applications integrated with language models </br>\n",
    "In this notebook, we shall learn about some of the core functionalities of LangChain and how you can use them </br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83d8d28",
   "metadata": {},
   "source": [
    "<b> Capabilities </b>\n",
    "\n",
    "1. Modelling interface: enables easy & streamlined access to language models from OpenAI, HuggingFace, etc\n",
    "\n",
    "2. Prompts: prompt management, prompt optimization & serialization. Easily create prompt templates\n",
    "\n",
    "3. Chains: Allows combining multiple tasks with or without language models to executed together\n",
    "\n",
    "4. Memory:\n",
    "\n",
    "5. Indexes: Combine model with your own custom data. It provides data loaders and vector stores\n",
    "\n",
    "6. Agent & Tools: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4240a8df",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai\n",
      "  Downloading openai-0.27.8-py3-none-any.whl (73 kB)\n",
      "     ---------------------------------------- 0.0/73.6 kB ? eta -:--:--\n",
      "     ---------------------------------------- 73.6/73.6 kB 4.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: tqdm in c:\\users\\sourav\\anaconda3\\envs\\genai\\lib\\site-packages (from openai) (4.65.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\sourav\\anaconda3\\envs\\genai\\lib\\site-packages (from openai) (3.8.3)\n",
      "Requirement already satisfied: requests>=2.20 in c:\\users\\sourav\\anaconda3\\envs\\genai\\lib\\site-packages (from openai) (2.29.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sourav\\anaconda3\\envs\\genai\\lib\\site-packages (from requests>=2.20->openai) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sourav\\anaconda3\\envs\\genai\\lib\\site-packages (from requests>=2.20->openai) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\sourav\\anaconda3\\envs\\genai\\lib\\site-packages (from requests>=2.20->openai) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sourav\\anaconda3\\envs\\genai\\lib\\site-packages (from requests>=2.20->openai) (2023.5.7)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\sourav\\anaconda3\\envs\\genai\\lib\\site-packages (from aiohttp->openai) (22.1.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\sourav\\anaconda3\\envs\\genai\\lib\\site-packages (from aiohttp->openai) (4.0.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\sourav\\anaconda3\\envs\\genai\\lib\\site-packages (from aiohttp->openai) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\sourav\\anaconda3\\envs\\genai\\lib\\site-packages (from aiohttp->openai) (1.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\sourav\\anaconda3\\envs\\genai\\lib\\site-packages (from aiohttp->openai) (6.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\sourav\\anaconda3\\envs\\genai\\lib\\site-packages (from aiohttp->openai) (1.8.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\sourav\\anaconda3\\envs\\genai\\lib\\site-packages (from tqdm->openai) (0.4.6)\n",
      "Installing collected packages: openai\n",
      "Successfully installed openai-0.27.8\n"
     ]
    }
   ],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c8cf37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "import os\n",
    "#import openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2edbbde0",
   "metadata": {},
   "source": [
    "#### API KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "02b03d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "········\n"
     ]
    }
   ],
   "source": [
    "from getpass import getpass\n",
    "HUGGINGFACEHUB_API_TOKEN = getpass()\n",
    "#os.environ['OPENAI_API_KEY'] = '...'\n",
    "os.environ['HUGGINGFACEHUB_API_TOKEN'] = HUGGINGFACEHUB_API_TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a843191",
   "metadata": {},
   "source": [
    "### 1. Modelling interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8e3f516c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#openai.api_key = os.environ['OPENAI_API_KEY']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "54ac67f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "import torch.nn.functional as F\n",
      "from\n"
     ]
    }
   ],
   "source": [
    "#from langchain.llms import OpenAI\n",
    "from langchain import HuggingFaceHub\n",
    "#llm = OpenAI(model_name=\"text-davinci-003\", )\n",
    "\n",
    "llm = HuggingFaceHub(repo_id='stabilityai/stablecode-completion-alpha-3b-4k')\n",
    "prompt = \"import torch\\nimport torch.nn as nn\"\n",
    "completion = llm(prompt)\n",
    "\n",
    "print(completion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1ff30e",
   "metadata": {},
   "source": [
    "You could use multiple LLM interfaces such as OpenAI, HuggingFaceHub etc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075d2975",
   "metadata": {},
   "source": [
    "### 2. Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "680f35a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Name a market leader that makes computers'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "template = \"Name a market leader that makes {product}\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=['product'],\n",
    "    template = template\n",
    ")\n",
    "\n",
    "prompt.format(product=\"computers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d7284223",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Write an email to your HR stating about your reason of absence from office for 5 days.\\nThe email should not exceed 50 words and must contain a subject'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "email_template = \"\"\"Write an email to your {receiver_name} stating about your reason of absence from office for {x} days.\n",
    "The email should not exceed 50 words and must contain a subject\"\"\"\n",
    "\n",
    "prompt2 = PromptTemplate(\n",
    "    input_variables=['receiver_name','x'],\n",
    "    template=email_template\n",
    ")\n",
    "\n",
    "prompt2.format(receiver_name=\"HR\", x=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ac9b8b",
   "metadata": {},
   "source": [
    "A good prompt is what makes your language models really useful. With the prompt templates you can organize and create standard prompt templates for generating information repeatedly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d84526a",
   "metadata": {},
   "source": [
    "### 3. Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7fc349c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' line written from the point of view of the employee concerned.\\nEmail the HR about the reason'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import LLMChain\n",
    "\n",
    "llm2 = HuggingFaceHub(repo_id='gpt2-large')\n",
    "chain = LLMChain(llm=llm2,\n",
    "                prompt=prompt2)\n",
    "\n",
    "chain.run(receiver_name=\"HR\",x=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7349fe79",
   "metadata": {},
   "source": [
    "### 4. Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4d4e74ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Do you know a good platform to learn data science?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Do you know a good platform to learn data science?\n",
      "AI:  What\n",
      "Human: The learners have been very supportive. Let's thank them\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Thank'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import ConversationChain\n",
    "\n",
    "conversation = ConversationChain(llm=llm2, verbose=True)\n",
    "conversation.predict(input=\"Do you know a good platform to learn data science?\")\n",
    "#conversation.predict(input=\"Sourav is regularly building content for Datahat\")\n",
    "conversation.predict(input=\"The learners have been very supportive. Let's thank them\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9958e1",
   "metadata": {},
   "source": [
    "### 5. Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624e51f0",
   "metadata": {},
   "source": [
    "Lack of contextual information such as access to particular documents or emails is one drawback of LLMs. </br>\n",
    "Giving LLM's access to particular external data will help build personalized context </br>\n",
    "</br>\n",
    "Langchain provides vector stores to store information as documents and index them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1653055",
   "metadata": {},
   "source": [
    "### 6. Agents & Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c15c922",
   "metadata": {},
   "source": [
    "Language models are trained on historical information, meaning they have no awareness of the present happenings </br>\n",
    "Imaging intergrating your language model with a search engine to cater to information in real time and generate responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c2e4fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
